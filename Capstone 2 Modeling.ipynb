{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to prepare and evaluate several different models. Each model will be run across a range of hyperparameter values. Also, each model will be applied to 4 sets of outcome and predictor variables. I'll predict DNAtype based on amino acids, DNAtype based on codons, kingdom based on amino acids, and kingdom based on codons. Incorporating both the amino acid and codon data in a single model would be unwarranted, as the amino acid frequencies are linear combinations of the codon frequencies.  I'll begin with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xg\n",
    "import matplotlib.pyplot as plt\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc, accuracy_score, log_loss, classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
    "import shap\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codon_list = ['UUU', 'UUC', 'UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG',\n",
    "       'AUU', 'AUC', 'AUA', 'AUG', 'GUU', 'GUC', 'GUA', 'GUG', 'GCU', 'GCC',\n",
    "       'GCA', 'GCG', 'CCU', 'CCC', 'CCA', 'CCG', 'UGG', 'GGU', 'GGC', 'GGA',\n",
    "       'GGG', 'UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC', 'ACU', 'ACC', 'ACA',\n",
    "       'ACG', 'UAU', 'UAC', 'CAA', 'CAG', 'AAU', 'AAC', 'UGU', 'UGC', 'CAU',\n",
    "       'CAC', 'AAA', 'AAG', 'CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG', 'GAU',\n",
    "       'GAC', 'GAA', 'GAG', 'UAA', 'UAG', 'UGA']\n",
    "\n",
    "amino_list = ['alanine', 'arginine',\n",
    "       'asparagine', 'aspartic acid', 'cysteine', 'glutamine', 'glutamic acid',\n",
    "       'glycine', 'histidine', 'isoleucine', 'leucine', 'lysine', 'methionine',\n",
    "       'phenylalanine', 'proline', 'serine', 'threonine', 'tryptophan',\n",
    "       'tyrosine', 'valine', 'start', 'stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnacols = ['D_chloroplast', 'D_genomic', 'D_mitochondrial']\n",
    "kingcols = ['K_bacteria', 'K_virus', 'K_plant', 'K_vertebrate', 'K_invertebrate',\n",
    "            'K_mammal', 'K_bacteriophage', 'K_rodent', 'K_primate', 'K_archaea']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the csv from the preprocessing notebook\n",
    "cu = pd.read_csv('codon_usage3.csv', index_col='Unnamed: 0')\n",
    "cu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# am having some problems with NA popping up after train_test_split!\n",
    "cu.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cu.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu['KingLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it could be useful to store the results for all of these models \n",
    "# some of these will be dict()s - which may be a challenge, but seems like the best solution\n",
    "# I hate having model_id as a column but I see no good alternative\n",
    "\n",
    "r_cols = ['model_id', 'model_type', 'inputs', 'outputs', 'hyperparameters', 'assessment', 'start_time', 'run_time'] \n",
    "results = pd.DataFrame(columns = r_cols)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it could also be interesting at this stage to try PCA:\n",
    "\n",
    "c = 8\n",
    "pca = PCA(n_components=c)\n",
    "pca_fit = pca.fit_transform(cu[codon_list].values)\n",
    "\n",
    "plt.plot(range(1,c+1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1,c+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component and Cumulative Explained Variance: codons\")\n",
    "\n",
    "for i in range(0,c):\n",
    "    print('component:', i+1, 'var exp:', pca.explained_variance_ratio_[i])\n",
    "\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I neglected to use StandardScaler() first. It's interesting to see just how much these variance components are inflated in the absence of standardization.  Let's try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "codon_ss = ss.fit_transform(cu[codon_list])\n",
    "\n",
    "c = 8\n",
    "pca = PCA(n_components=c)\n",
    "pca_fit = pca.fit_transform(codon_ss)\n",
    "\n",
    "plt.plot(range(1,c+1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1,c+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component and Cumulative Explained Variance, codons\")\n",
    "\n",
    "for i in range(0,c):\n",
    "    print('component:', i+1, 'var exp:', pca.explained_variance_ratio_[i])\n",
    "\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also try this on aminos\n",
    "\n",
    "ss = StandardScaler()\n",
    "amino_ss = ss.fit_transform(cu[amino_list])\n",
    "\n",
    "c = 8\n",
    "pca = PCA(n_components=c)\n",
    "pca_fit = pca.fit_transform(amino_ss)\n",
    "\n",
    "plt.plot(range(1,c+1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1,c+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component and Cumulative Explained Variance, aminos\")\n",
    "\n",
    "for i in range(0,c):\n",
    "    print('component:', i+1, 'var exp:', pca.explained_variance_ratio_[i])\n",
    "\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the 'elbow method' as one might in, say, k-means clustering, it suggests that applying models to just 3 principal components might be useful.  Still, this only explains only 69% of the variance - if we extend this graph to many components, it has a very long tail.  I'm unconvinced that it would really be beneficial to apply some kind of dimensionality reduction.  But I might return to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying some ML models\n",
    "\n",
    "I'm going to begin by setting aside a split for final testing across all models.  Because the models are being tested against four conditions (predicting DNAtype and kingdom by aminos and codons) this is a bit complicated.  Instead of creating four separate test split sets (each with X_test and y_test) I'm going to create a test set dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to store the training splits in the original cu dataframe to avoid having to recode everything.\n",
    "# But I'd like to retain the complete dataframe, it might prove useful.\n",
    "\n",
    "cufull=cu\n",
    "cufull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there were some problems with NaN's appearing for some reason\n",
    "cufull.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = codon_list + amino_list + ['Ncodons']\n",
    "all_Y = kingcols + dnacols + ['Kingdom', 'DNAtype', 'KingLabel', 'DNALabel']\n",
    "testset = pd.DataFrame(columns = all_X + all_Y)\n",
    "cu = pd.DataFrame(columns = all_X + all_Y)\n",
    "cu[all_X], testset[all_X], cu[all_Y], testset[all_Y] = train_test_split(cufull[all_X], cufull[all_Y], test_size=0.15, random_state=1371)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this checks out - I was having a persistent problem with NaN's appearing after the split.  It's not happening now, but I have no idea what was producing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NN might not be the model type most likely to prove useful, but it has only a single hyperparameter (k, the number of neighbors) to consider.  I'll incorporate everything I need to do into a simple function, this will help avoid tragic mistakes and simplify the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvts(X, y, val_p=0.15, test_p=0.15, rand=0):\n",
    "    '''Three way train-validation-test split function'''\n",
    "    first_split = val_p + test_p\n",
    "    second_split = test_p / first_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=first_split, random_state=rand)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=second_split, random_state=rand+1)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "outs = tvts(cu[amino_list], cu[kingcols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for out in outs:\n",
    "    print(len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_all(X_train, X_val, X_test):\n",
    "    '''uses StandardScaler() to fit_transform X_train, and then transform X_val and X_test'''\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    X_test = ss.transform(X_test)\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tv(X_train, X_val):\n",
    "    '''uses StandardScaler() to fit_transform X_train and transform X_val'''\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''It is EXTREMELY PAINFUL to me that it proved necessary to include Xn and yn to represent the NAMES of the \n",
    "parameters X and y in the function below.  I looked long and hard and ultimately in vain for a clean, simple way\n",
    "to obtain the name of a variable as a string.  Perhaps this is possible, but I was more concerned about getting\n",
    "the function to work.''' \n",
    "\n",
    "def knn_fit(y, X, yn='', Xn='', max_k=8, tts_random_state=0, test_id=0, do_plots=False):\n",
    "    '''Fits a k-nearest neighbors model and returns to results dataframe.'''\n",
    "    # split all data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])\n",
    "    \n",
    "    y_test = testset[y]\n",
    "    \n",
    "    output = pd.DataFrame(columns=r_cols)\n",
    "    train_a, val_a, test_a, train_f1, val_f1, test_f1, train_ra, val_ra, test_ra = ([] for i in range(9))\n",
    "    \n",
    "    for i in range(1, max_k+1):\n",
    "        starttime = datetime.now()\n",
    "        knn = KNeighborsClassifier(i)\n",
    "        knn.fit(X_train,y_train)\n",
    "     \n",
    "        train_pred, test_pred, val_pred = knn.predict(X_train), knn.predict(X_test), knn.predict(X_val)\n",
    "      \n",
    "        \n",
    "        train_prob_pred = knn.predict_proba(X_train)\n",
    "        test_prob_pred = knn.predict_proba(X_test)\n",
    "        val_prob_pred = knn.predict_proba(X_val)\n",
    "        \n",
    "        test_a.append(accuracy_score(y_test, test_pred))\n",
    "        train_a.append(accuracy_score(y_train, train_pred))\n",
    "        val_a.append(accuracy_score(y_val, val_pred))\n",
    "        \n",
    "        test_f1.append(f1_score(y_test, test_pred, average='micro'))\n",
    "        train_f1.append(f1_score(y_train, train_pred, average='micro'))\n",
    "        val_f1.append(f1_score(y_val, val_pred, average='micro'))\n",
    "        \n",
    "        # according to the sklearn docs, roc_auc requires predict_proba; this failed to work\n",
    "        test_ra.append(roc_auc_score(y_test, test_pred, average='weighted', multi_class='ovo'))\n",
    "        train_ra.append(roc_auc_score(y_train, train_pred, average='weighted', multi_class='ovo'))\n",
    "        val_ra.append(roc_auc_score(y_val, val_pred, average='weighted', multi_class='ovo'))\n",
    "        \n",
    "        item = [test_id, 'kNN', Xn, yn, {'k':i}, {'train_f1':train_f1[-1], 'val_f1':val_f1[-1], 'test_f1':test_f1[-1],  \n",
    "                                                  'train_accuracy':train_a[-1], 'val_accuracy':val_a[-1],'test_accuracy':test_a[-1], \n",
    "                                                  'train_roc_auc':test_ra[-1], 'val_roc_auc':val_ra[-1], 'test_roc_auc':test_ra[-1]},\n",
    "                                                    starttime, datetime.now()-starttime]\n",
    "        itemdict = dict(zip(r_cols, item))\n",
    "        output = output.append(itemdict, ignore_index=True)\n",
    "\n",
    "    if do_plots == True:\n",
    "        title = 'k-NN predicting ' + yn + ' using ' + Xn\n",
    "        line1, = plt.plot(range(1, max_k+1), train_a, 'b')\n",
    "        line2, = plt.plot(range(1, max_k+1), val_a, 'g')\n",
    "        line3, = plt.plot(range(1, max_k+1), test_a, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training accuracy', 'validation accuracy', 'testing accuracy'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('k')\n",
    "        plt.show() \n",
    "        \n",
    "        line1, = plt.plot(range(1, max_k+1), train_f1, 'b')\n",
    "        line2, = plt.plot(range(1, max_k+1), val_f1, 'g')\n",
    "        line3, = plt.plot(range(1, max_k+1), test_f1, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training F1', 'validation F1', 'testing F1'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('F1')\n",
    "        plt.xlabel('k')\n",
    "        plt.show() \n",
    "        \n",
    "        line1, = plt.plot(range(1, max_k+1), train_ra, 'b')\n",
    "        line2, = plt.plot(range(1, max_k+1), val_ra, 'g')\n",
    "        line3, = plt.plot(range(1, max_k+1), test_ra, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training roc_auc', 'validation roc_auc', 'testing roc_auc'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('roc_auc')\n",
    "        plt.xlabel('k')\n",
    "        plt.show() \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting dna on codon, dna on amino, kingdom on codon, kingdom on amino\n",
    "results=results.append(knn_fit(dnacols, codon_list, 'dna', 'codon', max_k=10, tts_random_state=535, test_id='1001001', do_plots=True), ignore_index=True)\n",
    "results=results.append(knn_fit(dnacols, amino_list, 'dna', 'amino', max_k=10, tts_random_state=77, test_id='1002001', do_plots=True), ignore_index=True)\n",
    "results=results.append(knn_fit(kingcols, codon_list, 'king', 'codon', max_k=10, tts_random_state=300, test_id='1003001', do_plots=True), ignore_index=True)\n",
    "results=results.append(knn_fit(kingcols, amino_list, 'king', 'amino', max_k=10, tts_random_state=416, test_id='1004001', do_plots=True), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results.assessment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these scores are higher than expected. It's interesting (and probably important) that higher assessment metrics were obtained in the models predicting DNA type than in the models predicting kingdom.  Perhaps the most noteworthy feature observable from these graphs is that, for the k-NN models predicting DNA type from the amino frequencies, **some of the test scores are higher than the train scores.**  As I understand it, this is a sure sign that something has gone glaringly wrong. What's happening here?  If there were some kind of problem in cleaning or preprocessing, wouldn't the same problem manifest itself in the other three test cases?\n",
    "\n",
    "**Update**: Having considered that perhaps there was some data leakage from the preprocessing notebook, I removed the StandardScaler() function from that stage and conducted it **after** the train-test split.  The results are the same.  Not understanding why this is happening is like a stone in my shoe, but I have to move on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['assessment'][4].get('train_f1')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as expected; the 'results' dataframe appears to be storing the results as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start with a single random forest model, and assess feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cufull[dnacols]\n",
    "X = cufull[codon_list]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=133)\n",
    "X_train, X_test = scale_tv(X_train, X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state = 1,n_jobs=-1)\n",
    "model_res = rfc.fit(X_train, y_train)\n",
    "y_pred = model_res.predict(X_test)\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "\n",
    "print('Random Forest: f1-score=%.3f' % (f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rfc.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cufull[dnacols]\n",
    "X = cufull[amino_list]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1863)\n",
    "X_train, X_test = scale_tv(X_train, X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state = 531,n_jobs=-1)\n",
    "model_res = rfc.fit(X_train, y_train)\n",
    "y_pred = model_res.predict(X_test)\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "\n",
    "print('Random Forest: f1-score=%.3f' % (f1))\n",
    "\n",
    "feature_importance = rfc.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cufull[kingcols]\n",
    "X = cufull[codon_list]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=133)\n",
    "X_train, X_test = scale_tv(X_train, X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state = 1,n_jobs=-1)\n",
    "model_res = rfc.fit(X_train, y_train)\n",
    "y_pred = model_res.predict(X_test)\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "\n",
    "print('Random Forest: f1-score=%.3f' % (f1))\n",
    "feature_importance = rfc.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cufull[kingcols]\n",
    "X = cufull[amino_list]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=154)\n",
    "X_train, X_test = scale_tv(X_train, X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state = 1,n_jobs=-1)\n",
    "model_res = rfc.fit(X_train, y_train)\n",
    "y_pred = model_res.predict(X_test)\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "\n",
    "print('Random Forest: f1-score=%.3f' % (f1))\n",
    "feature_importance = rfc.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As when examining the results of PCA:  it's interesting how quickly the relative importance drops off.  It's also worth noting that when trying to predict both DNA type and kingdom, 'CUA' stands out as the most prominent codon.  Perhaps something's up with this?  But let's examine hyperparameter tuning a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll try modifying the same function as above for k-NN\n",
    "\n",
    "def rfc_fit(y, X, yn='', Xn='', min_n_est=200, n_est_step=20, n_est_steps=11, tts_random_state=0, rfc_random_state=0, test_id=0, do_plots=False):\n",
    "    \n",
    "    # split all data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])\n",
    "    \n",
    "    y_test = testset[y]\n",
    "    \n",
    "    \n",
    "    train_a, val_a, test_a, train_f1, val_f1, test_f1, train_ra, val_ra, test_ra = ([] for i in range(9))\n",
    "    \n",
    "    output = pd.DataFrame(columns=r_cols)\n",
    "    \n",
    "    for i in range(n_est_steps):\n",
    "        starttime = datetime.now()\n",
    "        rfc = RandomForestClassifier(n_estimators=min_n_est+n_est_step*i, random_state=rfc_random_state, n_jobs=-1)\n",
    "        rfc.fit(X_train,y_train)\n",
    "     \n",
    "        train_pred = rfc.predict(X_train)\n",
    "        val_pred = rfc.predict(X_val)\n",
    "        test_pred = rfc.predict(X_test)\n",
    "        train_prob_pred = rfc.predict_proba(X_train)\n",
    "        val_prob_pred = rfc.predict_proba(X_val)\n",
    "        test_prob_pred = rfc.predict_proba(X_test)\n",
    "    \n",
    "        test_a.append(accuracy_score(y_test, test_pred))\n",
    "        val_a.append(accuracy_score(y_val, val_pred))\n",
    "        train_a.append(accuracy_score(y_train, train_pred))\n",
    "    \n",
    "        test_f1.append(f1_score(y_test, test_pred, average='micro'))\n",
    "        val_f1.append(f1_score(y_val, val_pred, average='micro'))\n",
    "        train_f1.append(f1_score(y_train, train_pred, average='micro'))\n",
    "        \n",
    "        # according to the sklearn docs, roc_auc requires predict_proba; this failed to work\n",
    "        test_ra.append(roc_auc_score(y_test, test_pred, average='weighted', multi_class='ovo'))\n",
    "        val_ra.append(roc_auc_score(y_val, val_pred, average='weighted', multi_class='ovo'))\n",
    "        train_ra.append(roc_auc_score(y_train, train_pred, average='weighted', multi_class='ovo'))\n",
    "    \n",
    "        \n",
    "    for i in range(n_est_steps):\n",
    "        item = [test_id, 'RFC', Xn, yn, {'n_estimators':min_n_est+n_est_step*i}, {'train_f1':train_f1[i], 'val_f1':val_f1[i], 'test_f1':test_f1[i],\n",
    "                                                  'train_accuracy':train_a[i], 'val_accuracy':val_a[i], 'test_accuracy':test_a[i], \n",
    "                                                  'train_roc_auc':test_ra[i], 'val_roc_auc':val_ra[i], 'test_roc_auc':test_ra[i]}, \n",
    "                                                    starttime, datetime.now()-starttime]\n",
    "        itemdict = dict(zip(r_cols, item))\n",
    "        output = output.append(itemdict, ignore_index=True)\n",
    "\n",
    "    if do_plots == True:\n",
    "        title = 'RFC predicting ' + yn + ' using ' + Xn\n",
    "        line1, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), train_a, 'b')\n",
    "        line2, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), val_a, 'g')\n",
    "        line3, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), test_a, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training accuracy', 'validation accuracy', 'testing accuracy'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('n_estimators')\n",
    "        plt.show() \n",
    "        \n",
    "        title = 'RFC predicting ' + yn + ' using ' + Xn\n",
    "        line1, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), train_f1, 'b')\n",
    "        line2, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), val_f1, 'g')\n",
    "        line3, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), test_f1, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training f1', 'validation f1', 'testing f1'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('f1')\n",
    "        plt.xlabel('n_estimators')\n",
    "        plt.show() \n",
    "        \n",
    "        title = 'RFC predicting ' + yn + ' using ' + Xn\n",
    "        line1, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), train_ra, 'b')\n",
    "        line2, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), val_ra, 'g')\n",
    "        line3, = plt.plot(range(min_n_est, min_n_est + n_est_step * n_est_steps, n_est_step), test_ra, 'r')\n",
    "        plt.legend([line1, line2, line3], ['training roc-auc', 'validation roc-auc', 'testing roc-auc'])\n",
    "        plt.title(title)\n",
    "        plt.ylabel('roc_auc')\n",
    "        plt.xlabel('n_estimators')\n",
    "        plt.show() \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results = results.append(rfc_fit(dnacols, codon_list, 'dna', 'codon', test_id='2001001', tts_random_state=55, do_plots=True), ignore_index=True)\n",
    "# y=DNAtype X=amino\n",
    "results = results.append(rfc_fit(dnacols, amino_list, 'dna', 'amino', test_id='2002001', tts_random_state=12, do_plots=True), ignore_index=True)\n",
    "# y=kingdom X=codon\n",
    "results = results.append(rfc_fit(kingcols, codon_list, 'kingdom', 'codon', test_id='2003001', tts_random_state=63, do_plots=True), ignore_index=True)\n",
    "# y=kingdom X=amino\n",
    "results = results.append(rfc_fit(kingcols, amino_list, 'kingdom', 'amino', test_id='2004001', tts_random_state=40, do_plots=True), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference seems to be that, when predicting kingdom as opposed to DNAtype, the testing accuracy drops substantially.  In all cases, the uniformly high training accuracy (at of very close to 1) is suspicious. I'm also a bit concerned that, in all cases, there's hardly any variability in the validation and testing metrics.  On the other hand, this makes a choice of random forest model kind of arbitrary for subsequent examination of SHAP values.  It doesn't seem to matter that much what value the n_estimators parameter takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap for random forest models\n",
    "\n",
    "\n",
    "def rfc_shap(y, X, yn='', Xn='', n_est=300, tts_random_state=0, rfc_random_state=0, test_id=0):\n",
    "    starttime = datetime.now()\n",
    "    \n",
    "    print(\"X:\", Xn)\n",
    "    print(\"y:\", yn)\n",
    "    # split all data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])\n",
    "    y_test = testset[y]\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=n_est, random_state=rfc_random_state, n_jobs=-1)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    shap_values = shap.TreeExplainer(rfc).shap_values(X_train)\n",
    "    plot_title = \"Predicting\" + yn + \"using\" + Xn\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"bar\")\n",
    "    # summary_plot returns an error for other plot types for multiclass\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"violin\")\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"dot\")\n",
    "    # inserting these shap values into the results dataframe\n",
    "    item = [test_id, 'RFC-SHAP', Xn, yn, {'n_estimators': n_est}, shap_values, starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results=results.append(rfc_shap(dnacols, codon_list, 'dna', 'codon', tts_random_state=55, test_id='2011001'))\n",
    "# y=DNAtype X=amino\n",
    "results=results.append(rfc_shap(dnacols, amino_list, 'dna', 'amino', tts_random_state=12, test_id='2012001'))\n",
    "# y=kingdom X=codon\n",
    "results=results.append(rfc_shap(kingcols, codon_list, 'kingdom', 'codon', tts_random_state=63, test_id='2013001'))\n",
    "# y=kingdom X=amino\n",
    "results=results.append(rfc_shap(kingcols, amino_list, 'kingdom', 'amino', tts_random_state=40, test_id='2014001'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be interesting to explore waterfall plots for a couple of specific observations as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84 rows is just what I expect so far.  Let's continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that XGBClassifier doesn't require, and indeed, can't accept one-hot encoding.  I had to go back to preprocessing and restore the original 'Kingdom' and 'DNAtype' columns.  Also, I discovered that XGBClassifier's own label encoder is deprecated. Eliminating warnings about this requires another import.  **Turns out that, although XGBClassifier still accepted the encoding, this didn't mute the warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_fit(y, X, yn='', Xn='', tts_random_state=0, test_id=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "    y_test = testset[y]\n",
    "    starttime = datetime.now()\n",
    "    xgb = xg.XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    train_pred = xgb.predict(X_train)\n",
    "    val_pred = xgb.predict(X_val)\n",
    "    test_pred = xgb.predict(X_test)\n",
    "        \n",
    "    train_prob_pred = xgb.predict_proba(X_train)\n",
    "    val_prob_pred = xgb.predict_proba(X_val)\n",
    "    test_prob_pred = xgb.predict_proba(X_test)\n",
    "    \n",
    "    test_a = accuracy_score(y_test, test_pred)\n",
    "    val_a = accuracy_score(y_val, val_pred)\n",
    "    train_a = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "    test_f1 = f1_score(y_test, test_pred, average='micro')\n",
    "    val_f1 = f1_score(y_val, val_pred, average='micro')\n",
    "    train_f1 = f1_score(y_train, train_pred, average='micro')\n",
    "\n",
    "    test_roc = roc_auc_score(y_test, test_prob_pred, average='weighted', multi_class='ovo')\n",
    "    val_roc = roc_auc_score(y_val, val_prob_pred, average='weighted', multi_class='ovo')\n",
    "    train_roc = roc_auc_score(y_train, train_prob_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "\n",
    "    item = [test_id, 'XGB', Xn, yn, {'hyperparameters':'XGBClassifier defaults'}, \n",
    "                                {'train_accuracy':train_a, 'val_accuracy':val_a, 'test_accuracy':test_a, \n",
    "                                 'train_f1':train_f1, 'val_f1':val_f1, 'test_f1':test_f1,\n",
    "                                 'train_roc_auc':train_roc, 'val_roc_auc':val_roc, 'test_roc_auc':test_roc}, \n",
    "                                starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results = results.append(xgb_fit('DNALabel', codon_list, 'dna', 'codon', test_id='3001001', tts_random_state=6433), ignore_index=True)\n",
    "# y=DNAtype X=amino\n",
    "results = results.append(xgb_fit('DNALabel', amino_list, 'dna', 'amino', test_id='3002001', tts_random_state=1027), ignore_index=True)\n",
    "# y=kingdom X=codon\n",
    "results = results.append(xgb_fit('KingLabel', codon_list, 'kingdom', 'codon', test_id='3003001', tts_random_state=1693), ignore_index=True)\n",
    "# y=kingdom X=amino\n",
    "results = results.append(xgb_fit('KingLabel', amino_list, 'kingdom', 'amino', test_id='3004001', tts_random_state=7400), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also run the SHAP TreeExplainer on these XGBoost models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_shap(y, X, yn='', Xn='', tts_random_state=0, test_id=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "\n",
    "    xgb = xg.XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    shap_values = shap.TreeExplainer(xgb).shap_values(X_train)\n",
    "    plot_title = \"Predicting\" + yn + \"using\" + Xn\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"bar\")\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"violin\")\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"dot\")\n",
    "    # inserting these shap values into the results dataframe\n",
    "    item = [test_id, 'XGB-SHAP', Xn, yn, {'hyperparameters': 'XGB defaults'}, shap_values, \n",
    "                                                    starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.append(xgb_shap('DNALabel', codon_list, 'dna', 'codon', tts_random_state=6433, test_id='3011001'))\n",
    "results=results.append(xgb_shap('DNALabel', amino_list, 'dna', 'amino', tts_random_state=1027, test_id='3012001'))\n",
    "results=results.append(xgb_shap('KingLabel', codon_list, 'king', 'codon', tts_random_state=1693, test_id='3013001'))\n",
    "results=results.append(xgb_shap('KingLabel', amino_list, 'king', 'amino', tts_random_state=7400, test_id='3014001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.iloc[-1].assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what to get out of this.  At least I got XGBoost to work; there are so many hyperparameters, though, that I'm not sure how to go about tuning them.  Nor am I sure how to evaluate these models, other than by such metrics as accuracy, F1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier using Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyper_space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# along the same lines, since I'm going to be doing this for four models, I'm going to wrap it in a function\n",
    "def xgb_hyper_fit(y, X, yn='', Xn='', tts_random_state=0, space=xgb_hyper_space, test_id=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "    y_test = testset[y]\n",
    "    starttime = datetime.now()\n",
    "    def objective(space):\n",
    "        xgb=xg.XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "    \n",
    "        evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "    \n",
    "        xgb.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "    \n",
    "        train_pred = xgb.predict(X_train)\n",
    "        val_pred = xgb.predict(X_val)\n",
    "        test_pred = xgb.predict(X_test)\n",
    "        \n",
    "        train_prob_pred = xgb.predict_proba(X_train)\n",
    "        val_prob_pred = xgb.predict_proba(X_val)\n",
    "        test_prob_pred = xgb.predict_proba(X_test)\n",
    "    \n",
    "        test_a = accuracy_score(y_test, test_pred)\n",
    "        val_a = accuracy_score(y_val, val_pred)\n",
    "        train_a = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "        test_f1 = f1_score(y_test, test_pred, average='micro')\n",
    "        val_f1 = f1_score(y_val, val_pred, average='micro')\n",
    "        train_f1 = f1_score(y_train, train_pred, average='micro')\n",
    "\n",
    "        test_roc = roc_auc_score(y_test, test_prob_pred, average='weighted', multi_class='ovo')\n",
    "        val_roc = roc_auc_score(y_val, val_prob_pred, average='weighted', multi_class='ovo')\n",
    "        train_roc = roc_auc_score(y_train, train_prob_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "        return {'loss': -val_f1, 'train_accuracy': train_a, 'val_accuracy': val_a, 'test_accuracy': test_a,\n",
    "            'train_f1': train_f1, 'validation_f1': val_f1, 'test_f1': test_f1, \n",
    "            'train_roc_auc': train_roc, 'val_roc_auc':val_roc, 'test_roc_auc':test_roc, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best_hyperparams = hyperopt.fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = hyperopt.tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "    \n",
    "    assess = trials.best_trial['result']\n",
    "    del assess['loss']\n",
    "    del assess['status']\n",
    "    \n",
    "    item = [test_id, 'XGB-Hyperopt', Xn, yn, best_hyperparams, assess, starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results = results.append(xgb_hyper_fit('DNALabel', codon_list, 'dna', 'codon', test_id='3101001', tts_random_state=3913), ignore_index=True)\n",
    "# y=DNAtype X=amino\n",
    "results = results.append(xgb_hyper_fit('DNALabel', amino_list, 'dna', 'amino', test_id='3102001', tts_random_state=7003), ignore_index=True)\n",
    "# y=kingdom X=codon\n",
    "results = results.append(xgb_hyper_fit('KingLabel', codon_list, 'kingdom', 'codon', test_id='3103001', tts_random_state=3904), ignore_index=True)\n",
    "# y=kingdom X=amino\n",
    "results = results.append(xgb_hyper_fit('KingLabel', amino_list, 'kingdom', 'amino', test_id='3104001', tts_random_state=6492), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there's a lot to explore in the MLPClassifier object; I need to read a lot more documentation.  Let's see if I can implement hyperopt over some of the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyper_space={'hidden_layer_sizes': hp.quniform(\"hidden_layer_sizes\", 50, 150, 1),\n",
    "       'activation': hp.choice('activation', ['relu', 'tanh', 'logistic']),\n",
    "        'learning_rate_init': hp.uniform('learning_rate_init', 0.0001,0.01)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# along the same lines, since I'm going to be doing this for four models, I'm going to wrap it in a function\n",
    "def mlp_hyper_fit(y, X, yn='', Xn='', tts_random_state=0, space=mlp_hyper_space, test_id=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "    y_test = testset[y]\n",
    "    starttime = datetime.now()\n",
    "    def objective(space):\n",
    "        mlp = MLPClassifier(\n",
    "                    hidden_layer_sizes = int(space['hidden_layer_sizes']),\n",
    "                    activation = space['activation'],\n",
    "                    learning_rate_init = space['learning_rate_init'])\n",
    "    \n",
    "    \n",
    "        evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "    \n",
    "        mlp.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "        train_pred = mlp.predict(X_train)\n",
    "        val_pred = mlp.predict(X_val)\n",
    "        test_pred = mlp.predict(X_test)\n",
    "        \n",
    "        train_prob_pred = mlp.predict_proba(X_train)\n",
    "        val_prob_pred = mlp.predict_proba(X_val)\n",
    "        test_prob_pred = mlp.predict_proba(X_test)\n",
    "    \n",
    "        test_a = accuracy_score(y_test, test_pred)\n",
    "        val_a = accuracy_score(y_val, val_pred)\n",
    "        train_a = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "        test_f1 = f1_score(y_test, test_pred, average='micro')\n",
    "        val_f1 = f1_score(y_val, val_pred, average='micro')\n",
    "        train_f1 = f1_score(y_train, train_pred, average='micro')\n",
    "\n",
    "        test_roc = roc_auc_score(y_test, test_prob_pred, average='weighted', multi_class='ovo')\n",
    "        val_roc = roc_auc_score(y_val, val_prob_pred, average='weighted', multi_class='ovo')\n",
    "        train_roc = roc_auc_score(y_train, train_prob_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "        return {'loss': -val_f1, 'train_accuracy': train_a, 'val_accuracy': val_a, 'test_accuracy': test_a,\n",
    "            'train_f1': train_f1, 'validation_f1': val_f1, 'test_f1': test_f1, \n",
    "            'train_roc_auc': train_roc, 'val_roc_auc':val_roc, 'test_roc_auc':test_roc, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best_hyperparams = hyperopt.fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = hyperopt.tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "    \n",
    "    assess = trials.best_trial['result']\n",
    "    del assess['loss']\n",
    "    del assess['status']\n",
    "    \n",
    "    item = [test_id, 'MLP-Hyperopt', Xn, yn, best_hyperparams, assess, starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results = results.append(mlp_hyper_fit('DNALabel', codon_list, 'dna', 'codon', test_id='4101001', tts_random_state=2302), ignore_index=True)\n",
    "# y=DNAtype X=amino\n",
    "results = results.append(mlp_hyper_fit('DNALabel', amino_list, 'dna', 'amino', test_id='4102001', tts_random_state=6613), ignore_index=True)\n",
    "# y=kingdom X=codon\n",
    "results = results.append(mlp_hyper_fit('KingLabel', codon_list, 'kingdom', 'codon', test_id='4103001', tts_random_state=3824), ignore_index=True)\n",
    "# y=kingdom X=amino\n",
    "results = results.append(mlp_hyper_fit('KingLabel', amino_list, 'kingdom', 'amino', test_id='4104001', tts_random_state=7081), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a shockingly long time to run, and I was only including three hyperparameters in the space.  Perhaps more troubling, the 'best hyperparameters' included a value of zero for 'activation' - this isn't one of the values I provided in the space dictionary, NOR is it an admissible parameter value for MLPClassifier, so this is just baffling. It's interesting that absolutely all of the F1 scores listed are very high, regardless of the values chosen by hyperopt.   I'm also uncertain how to isolate the 'best hyperparameters' model so as to get measures like its accuracy and F1, make and assess predictions based on new data, etc.  However, for now, I'm going to see how this works out using amino_list as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLPClassifier model can be explored using shap.DeepExplainer, or so I've read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not using tensorflow at all but shap DeepExplainer threw an error that there was no such module... ominous...\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_shap(y, X, yn='', Xn='', tts_random_state=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "    \n",
    "    # shap DeepExplainer needs a 'data' parameter\n",
    "    # docs caution against making it very big...\n",
    "    # this is not ideal but let's see if it works\n",
    "    # X_tr, X_shap, y_tr, y_shap = train_test_split(X_train, y_train, test_size=0.2, random_state=tts_random_state+1)\n",
    "\n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    shap_values = shap.KernelExplainer(mlp.predict_proba, X_train[:50,:]).shap_values(X_train[:299,:])\n",
    "    plot_title = \"Predicting\" + yn + \"using\" + Xn\n",
    "    shap.summary_plot(shap_values, X_train[:299,:], feature_names=X, title=plot_title, plot_type=\"bar\")\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"violin\")\n",
    "    #shap.summary_plot(shap_values, X_train, feature_names=X, title=plot_title, plot_type=\"dot\")\n",
    "    item = [test_id, 'MLP-SHAP', Xn, yn, {'hyperparameters': 'MLP defaults'}, shap_values, starttime, datetime.now()-starttime]\n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.append(mlp_shap('DNALabel', codon_list, 'dna', 'codon', tts_random_state=2302, test_id='4011001'))\n",
    "results=results.append(mlp_shap('DNALabel', amino_list, 'dna', 'amino', tts_random_state=6613, test_id='4012001'))\n",
    "results=results.append(mlp_shap('KingLabel', codon_list, 'king', 'codon', tts_random_state=3824, test_id='4013001'))\n",
    "results=results.append(mlp_shap('KingLabel', amino_list, 'king', 'amino', tts_random_state=7081, test_id='4014001'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note before beginning that all naive Bayes models make the assumption of independence among predictor variables.  **We can know to a certainty that this is not the case here**, but, looking at examples of successful applications of naive Bayes models, most of them don't really have independence either.  Nevertheless, such models have proven somewhat successful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yet again, for gaussian naive bayes - not a lot of hyperparameters\n",
    "\n",
    "def gnb_fit(y, X, yn='', Xn='', tts_random_state=0, test_id=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(cu[X], cu[y], test_size=0.15, random_state=tts_random_state)\n",
    "    # scale X data\n",
    "    X_train, X_val, X_test = scale_all(X_train, X_val, testset[X])    \n",
    "    y_test = testset[y]\n",
    "    starttime = datetime.now()\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    train_pred = gnb.predict(X_train)\n",
    "    val_pred = gnb.predict(X_val)\n",
    "    test_pred = gnb.predict(X_test)\n",
    "        \n",
    "    train_prob_pred = gnb.predict_proba(X_train)\n",
    "    val_prob_pred = gnb.predict_proba(X_val)\n",
    "    test_prob_pred = gnb.predict_proba(X_test)\n",
    "    \n",
    "    test_a = accuracy_score(y_test, test_pred)\n",
    "    val_a = accuracy_score(y_val, val_pred)\n",
    "    train_a = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "    test_f1 = f1_score(y_test, test_pred, average='micro')\n",
    "    val_f1 = f1_score(y_val, val_pred, average='micro')\n",
    "    train_f1 = f1_score(y_train, train_pred, average='micro')\n",
    "\n",
    "    test_roc = roc_auc_score(y_test, test_prob_pred, average='weighted', multi_class='ovo')\n",
    "    val_roc = roc_auc_score(y_val, val_prob_pred, average='weighted', multi_class='ovo')\n",
    "    train_roc = roc_auc_score(y_train, train_prob_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "\n",
    "    item = [test_id, 'GNB', Xn, yn, {'hyperparameters':'GaussianNB defaults'}, \n",
    "                                {'train_accuracy':train_a, 'val_accuracy':val_a, 'test_accuracy':test_a, \n",
    "                                 'train_f1':train_f1, 'val_f1':val_f1, 'test_f1':test_f1,\n",
    "                                 'train_roc_auc':train_roc, 'val_roc_auc':val_roc, 'test_roc_auc':test_roc}, \n",
    "                                starttime, datetime.now()-starttime]\n",
    "    \n",
    "    itemdict = dict(zip(r_cols, item))\n",
    "    return itemdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function on:\n",
    "# y=DNAtype X=codon\n",
    "results = results.append(gnb_fit('DNALabel', codon_list, 'dna', 'codon', test_id='5001001', tts_random_state=80223), ignore_index=True)\n",
    "# y=DNAtype X=amino\n",
    "results = results.append(gnb_fit('DNALabel', amino_list, 'dna', 'amino', test_id='5002001', tts_random_state=14097), ignore_index=True)\n",
    "# y=kingdom X=codon\n",
    "results = results.append(gnb_fit('KingLabel', codon_list, 'kingdom', 'codon', test_id='5003001', tts_random_state=62203), ignore_index=True)\n",
    "# y=kingdom X=amino\n",
    "results = results.append(gnb_fit('KingLabel', amino_list, 'kingdom', 'amino', test_id='5004001', tts_random_state=40331), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That, I think, is sufficient. To summarize:  I've estimated five families of models - K-nearest neighbors, random forest, extreme gradient boosting, multi-level perceptron classifiers, and gaussian naive bayes.  For each of these, four models were estimated, for each combination of two outcomes and two sets of features.  I'd like to compare them in a separate notebook, so I'm going to export the results dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this failed to work - trials object became string\n",
    "# results.to_csv(\"results1.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle(\"results1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
